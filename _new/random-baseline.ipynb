{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-18 12:10:19,221 | tokenizer | INFO : Loading spacy model\n",
      "2017-12-18 12:10:20,410 | tokenizer | INFO : Model Loaded\n"
     ]
    }
   ],
   "source": [
    "import db\n",
    "import models\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizer import Tokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "pd.options.display.max_colwidth = 0\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s | %(name)s | %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_id = 9\n",
    "n_tweets = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Documentos vs Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener **documentos** del evento `event_id` con solo los representantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event = db.get_documents(event_id, full=False)\n",
    "tweets = np.array(list(map(lambda x: x[0].text, event.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Death toll rises to at least 876 in earthquake that hit Nepal http://t.co/HrsU8nRwgv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#NepalQuake death toll tops 1,400, according to Nepal's National Emergency Operation Center. http://t.co/w3DV9S1oqD http://t.co/CXAxG6Zv27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sending my love to those in Nepal and everyone affected by this earthquake.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCIAF launch Nepal Earthquake Emergency Appeal http://t.co/syoyXSBqdk  @HumzaYousaf @NicolaSturgeon http://t.co/L2LBsLd883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terrible #earthquake in #Nepal. Just saved ourselves. Don't know how many killed. Roads are blocked already. http://t.co/qsxITz0n1P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Earthquakes kill scores in Nepal and India; historic Dharahara Tower collapses, Kathmandu airport shut\\n#earthquake\\nhttp://t.co/HUTZi3jpAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Facebook Safety Check connects those affected by devastating Nepal earthquake. http://t.co/lRiHiu0pdv http://t.co/N1gCs5PL1g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>earthquake in Nepal is about 7.8 M may ALLAH bless them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#ModiMinistry Quake magnitude upgraded to 7.9, only 2km deep http://t.co/dmXQOqp7IM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#PrayForNepal A 7.9 earthquake has killed more than 480 people. Pls join me in praying for #Nepal http://t.co/pHmrRMb5AG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             0\n",
       "0  Death toll rises to at least 876 in earthquake that hit Nepal http://t.co/HrsU8nRwgv                                                       \n",
       "1  #NepalQuake death toll tops 1,400, according to Nepal's National Emergency Operation Center. http://t.co/w3DV9S1oqD http://t.co/CXAxG6Zv27 \n",
       "2  Sending my love to those in Nepal and everyone affected by this earthquake.                                                                \n",
       "3  SCIAF launch Nepal Earthquake Emergency Appeal http://t.co/syoyXSBqdk  @HumzaYousaf @NicolaSturgeon http://t.co/L2LBsLd883                 \n",
       "4  Terrible #earthquake in #Nepal. Just saved ourselves. Don't know how many killed. Roads are blocked already. http://t.co/qsxITz0n1P        \n",
       "5  Earthquakes kill scores in Nepal and India; historic Dharahara Tower collapses, Kathmandu airport shut\\n#earthquake\\nhttp://t.co/HUTZi3jpAQ\n",
       "6  Facebook Safety Check connects those affected by devastating Nepal earthquake. http://t.co/lRiHiu0pdv http://t.co/N1gCs5PL1g               \n",
       "7  earthquake in Nepal is about 7.8 M may ALLAH bless them                                                                                    \n",
       "8  #ModiMinistry Quake magnitude upgraded to 7.9, only 2km deep http://t.co/dmXQOqp7IM                                                        \n",
       "9  #PrayForNepal A 7.9 earthquake has killed more than 480 people. Pls join me in praying for #Nepal http://t.co/pHmrRMb5AG                   "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.choice(tweets, n_tweets))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener **tweets** del evento `events_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event2 = db.get_tweets(event_id)\n",
    "tweets = np.array(list(map(lambda t: t.text, event2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At least 688 killed in Nepal earthquake, official says. http://t.co/E8Fh03tnSi http://t.co/kunrRmzMV2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appeal for relief of Nepal Earth quake victims http://t.co/KUZ2tS7hlw  #Nepal #Earthquake #SevaBharathi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google has launched a person finder tool to help locate those affected by the earthquake in Nepal. http://t.co/zJTZO9uenm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#PrayforKathmandu a man being rescued after a massive #Earthquake in #Nepal #NepalEarthquake all we can do is #Pray http://t.co/VmhYEu2Fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.5 magnitude earthquake hits Nepal, tremors felt in India, Pakistan\\nThe quake hit around noon on Saturday and... http://t.co/dmJqEhdUr5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BBC News - Nepal earthquake: Hundreds die, many feared trapped http://t.co/HnxkT0GbaL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MORE: Nepal police say at least 1,130 dead in massive earthquake centered outside of Kathmandu: http://t.co/SaRRmdOzgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GoI did a good job. The arrangements were good too: Anand Rao (Indian rescued from Nepal #earthquake) http://t.co/sQEGrnu6Hh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Israel News | At least 450 died in a Nepal earthquake - JerusalemOnline http://t.co/AmxGDYp0ZZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Praying for everyone affected by the earthquake in Nepal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            0\n",
       "0  At least 688 killed in Nepal earthquake, official says. http://t.co/E8Fh03tnSi http://t.co/kunrRmzMV2                                     \n",
       "1  Appeal for relief of Nepal Earth quake victims http://t.co/KUZ2tS7hlw  #Nepal #Earthquake #SevaBharathi                                   \n",
       "2  Google has launched a person finder tool to help locate those affected by the earthquake in Nepal. http://t.co/zJTZO9uenm                 \n",
       "3  #PrayforKathmandu a man being rescued after a massive #Earthquake in #Nepal #NepalEarthquake all we can do is #Pray http://t.co/VmhYEu2Fe7\n",
       "4  7.5 magnitude earthquake hits Nepal, tremors felt in India, Pakistan\\nThe quake hit around noon on Saturday and... http://t.co/dmJqEhdUr5 \n",
       "5  BBC News - Nepal earthquake: Hundreds die, many feared trapped http://t.co/HnxkT0GbaL                                                     \n",
       "6  MORE: Nepal police say at least 1,130 dead in massive earthquake centered outside of Kathmandu: http://t.co/SaRRmdOzgs                    \n",
       "7  GoI did a good job. The arrangements were good too: Anand Rao (Indian rescued from Nepal #earthquake) http://t.co/sQEGrnu6Hh              \n",
       "8  Israel News | At least 450 died in a Nepal earthquake - JerusalemOnline http://t.co/AmxGDYp0ZZ                                            \n",
       "9  Praying for everyone affected by the earthquake in Nepal                                                                                  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.choice(tweets, n_tweets))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Word Embeddings\n",
    "\n",
    "## Average embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-18 16:33:28,552 | summa.preprocessing.cleaner | INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-12-18 16:33:28,563 | gensim.models.keyedvectors | INFO : loading projection weights from /home/mquezada/phd/multimedia-summarization/data/word_embeddings/ft_alltweets_model.vec\n",
      "2017-12-18 16:34:42,727 | gensim.models.keyedvectors | INFO : loaded (1076139, 100) matrix from /home/mquezada/phd/multimedia-summarization/data/word_embeddings/ft_alltweets_model.vec\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "we = Path('/home/mquezada/phd/multimedia-summarization/data/word_embeddings/ft_alltweets_model.vec')\n",
    "model = KeyedVectors.load_word2vec_format(we.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_id = 8\n",
    "\n",
    "event = db.get_documents(event_id, full=False)\n",
    "tweets = np.array(list(map(lambda x: ' '.join(x[0].text.split()), event.values())))\n",
    "#tweets = np.array(list(event.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected = np.random.choice(tweets, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tweets).to_csv('texts.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-5ffd1e3f1b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtweet_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mtweet_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "vocabs = []\n",
    "\n",
    "for tweet_list in tqdm(selected):\n",
    "    tweet_vocab = set()\n",
    "    for tweet in tweet_list:\n",
    "        for token in tokenizer.tokenize(tweet.text):\n",
    "            tweet_vocab.add(token)\n",
    "    vocabs.append(tweet_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets escogidos a mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gunmen possibly linked to Islamic State attack hotel popular with foreigners in Libyan capital Tripoli -officials http://t.co/dNG1ykqJi0'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('libya_selected.tsv') as f:\n",
    "    labels, selected = [], []\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        labels.append(tokens[0])\n",
    "        selected.append(' '.join(tokens[1:]))\n",
    "\n",
    "selected[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 151.15it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "texts = []\n",
    "for label, tweet in tqdm(zip(labels, selected)):\n",
    "    vector = []\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        if token in model:\n",
    "            vector.append(model[token])\n",
    "    vector = np.mean(vector, axis=0)\n",
    "    if not np.isnan(vector).any():\n",
    "        vectors.append(vector)\n",
    "        texts.append((tweet, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gunmen possibly linked to Islamic State attack hotel popular with foreigners in Libyan capital Tripoli -officials http://t.co/dNG1ykqJi0', '1')\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(texts).to_csv('libya_labels_avg.tsv', sep='\\t', header=False, index=False)\n",
    "pd.DataFrame(vectors).to_csv('libya_vectors_avg.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Discourse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freqs = Counter()\n",
    "with open('/home/mquezada/phd/multimedia-summarization/data/word_embeddings/wordfrequencies_relative.tsv') as f:\n",
    "    for line in f:\n",
    "        word, freq = line.split()\n",
    "        freqs[word] = float(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discourse\n",
    "\n",
    "def discourse(labels, texts, alpha):\n",
    "    vectors = list()\n",
    "    final_labels = list()\n",
    "    final_texts = list()\n",
    "    \n",
    "    for label, text in tqdm(zip(labels, texts)):\n",
    "        tweet_vector = []\n",
    "        for token in tokenizer.tokenize(text):\n",
    "            if token in model and token in freqs:\n",
    "                vector = model[token]\n",
    "                prob = freqs[token]\n",
    "                tweet_vector.append((alpha / (alpha + prob)) * vector)\n",
    "\n",
    "        if tweet_vector:\n",
    "            vectors.append(np.mean(tweet_vector, axis=0))\n",
    "            final_labels.append(label)            \n",
    "            final_texts.append(text)\n",
    "            \n",
    "    #pca = PCA(n_components=1)\n",
    "    #pca.fit(np.array(list(vectors.values())))\n",
    "    #u = pca.components_\n",
    "    \n",
    "    final_vectors = []\n",
    "    for vector in tqdm(vectors):\n",
    "        final_vectors.append(vector) # - u.T.dot(u).dot(vector))\n",
    "        \n",
    "    return final_labels, final_texts, final_vectors\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 162.11it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 491079.74it/s]\n"
     ]
    }
   ],
   "source": [
    "disc_l, disc_t, disc_v = discourse(labels, selected, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '6']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 33928.20it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "rows = []\n",
    "for label, text, vector in tqdm(zip(disc_l, disc_t, disc_v)):\n",
    "    row = []\n",
    "    for dim in vector:\n",
    "        row.append(dim)\n",
    "    rows.append(row)\n",
    "    texts.append((text, label))\n",
    "    \n",
    "vectors = pd.DataFrame(rows)\n",
    "\n",
    "pd.DataFrame(texts).to_csv('libya_labels_disc.tsv', sep='\\t', header=False, index=False)\n",
    "pd.DataFrame(vectors).to_csv('libya_vectors_disc.tsv', sep='\\t', header=False, index=False)\n",
    "\n",
    "#d.to_csv('sample_100_disc.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sorted(freqs.items(), key=lambda x: x[1])\n",
    "\n",
    "with open('freqs.tsv', 'w') as f:\n",
    "    for w, fr in freqs.items():\n",
    "        f.write(f'{repr(w)}\\t{fr}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos completos\n",
    "\n",
    "## Libya hotel\n",
    "\n",
    "Todos los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28640,)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event2 = db.get_tweets(event_id)\n",
    "tweets = np.array(list(map(lambda t: t.text, event2)))\n",
    "\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remover duplicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join([token for token in tokenizer.tokenize(t)]) for t in tweets]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_set = set(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28640\n",
      "6975\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(len(texts_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6975/6975 [00:00<00:00, 24017.23it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "final_texts = []\n",
    "for t in tqdm(texts_set):\n",
    "    vector = [model[token] for token in t.split() if token in model]\n",
    "    if vector:\n",
    "        vectors.append(np.mean(vector, axis=0))\n",
    "        final_texts.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6972\n",
      "6972\n"
     ]
    }
   ],
   "source": [
    "print(len(vectors))\n",
    "print(len(final_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(final_texts).to_csv('libya_labels_avg_full.tsv', sep='\\t', header=False, index=False)\n",
    "pd.DataFrame(vectors).to_csv('libya_vectors_avg_full.tsv', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discourse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6975/6975 [00:00<00:00, 17208.63it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = list()\n",
    "final_texts = list()\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "for text in tqdm(texts_set):\n",
    "    tweet_vector = []\n",
    "    for token in text.split():\n",
    "        if token in model and token in freqs:\n",
    "            vector = model[token]\n",
    "            prob = freqs[token]\n",
    "            tweet_vector.append((alpha / (alpha + prob)) * vector)\n",
    "\n",
    "    if tweet_vector:\n",
    "        vectors.append(np.mean(tweet_vector, axis=0))\n",
    "        final_texts.append(text)\n",
    "\n",
    "pd.DataFrame(final_texts).to_csv('libya_labels_disc_full.tsv', sep='\\t', header=False, index=False)\n",
    "pd.DataFrame(vectors).to_csv('libya_vectors_disc_full.tsv', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg fasttext con los documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_texts, vectors = [], []\n",
    "\n",
    "with open('texts.tsv') as f:\n",
    "    for line in f:\n",
    "        vector = [model[token] for token in tokenizer.tokenize(line) if token in model]\n",
    "        if vector:\n",
    "            final_texts.append(line)\n",
    "            vectors.append(np.mean(vector, axis=0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_texts).to_csv('libya_labels_avg_docs.tsv', sep='\\t', header=False, index=False)\n",
    "pd.DataFrame(vectors).to_csv('libya_vectors_avg_docs.tsv', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
